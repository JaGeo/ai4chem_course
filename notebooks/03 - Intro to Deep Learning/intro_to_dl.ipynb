{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5e768a-9b68-48c6-9af0-9281a9762d6f",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/schwallergroup/ai4chem_course/blob/main/notebooks/03%20-%20Intro%20to%20Deep%20Learning/intro_to_dl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bae14-53f5-4adf-a28c-d8658dd8cd85",
   "metadata": {},
   "source": [
    "# Week 3 tutorial - AI 4 Chemistry\n",
    "\n",
    "## Table of content\n",
    "\n",
    "1. Supervised deep learning.\n",
    "2. Inductive biases.\n",
    "3. Training neural networks.\n",
    "4. Model selection and optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b17b43",
   "metadata": {},
   "source": [
    "# 0. Relevant packages\n",
    "\n",
    "### Pytorch\n",
    "Based on the Torch library, PyTorch is one of the most popular deep learning frameworks for machine learning practitioners. We will learn to use PyTorch to do deep learning work. You can also browse the PyTorch [tutorials](https://pytorch.org/tutorials/) and [docs](https://pytorch.org/docs/stable/index.html) for additional details.\n",
    "\n",
    "### Pytorch Lightning\n",
    "PyTorch Lightning is the deep learning framework for professional AI researchers and machine learning engineers who need maximal flexibility without sacrificing performance at scale. You can also browse its [documentation](https://pytorch-lightning.readthedocs.io/en/stable/) for additional details.\n",
    "\n",
    "### Pytorch Geometric (PyG)\n",
    "PyG is a library built upon PyTorch to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data. You can also browse its [documentation](https://pytorch-geometric.readthedocs.io/en/latest/) for additional details.\n",
    "\n",
    "### Weights & Biases (W&B)\n",
    "Weights & Biases is the machine learning platform for developers to build better models faster. Use W&B's lightweight, interoperable tools to quickly track experiments, version and iterate on datasets, evaluate model performance, reproduce models, visualize results and spot regressions, and share findings with colleagues. You can also browse its [documentation](https://docs.wandb.ai/) for additional details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ebe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all libraries\n",
    "# CoLab has already preinstalled Pytorch for you\n",
    "! pip install pytorch-lightning wandb rdkit ogb deepchem\n",
    "# install PyG\n",
    "import torch\n",
    "VERSION = torch.__version__\n",
    "! pip install pyg_lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-{VERSION}.html\n",
    "! pip install torch-geometric\n",
    "\n",
    "# Download all data\n",
    "! mkdir data/\n",
    "! wget https://raw.githubusercontent.com/schwallergroup/ai4chem_course/main/notebooks/02%20-%20Supervised%20Learning/data/esol.csv -O data/esol.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5e52b4-30d8-4940-9d54-e71e5ece96ce",
   "metadata": {},
   "source": [
    "# 1. Supervised Deep Learning\n",
    "\n",
    "From last session we should already be familiar with supervised learning: is a type of machine learning that involves training a model on a labeled dataset to learn the relationships between input and output data.\n",
    "\n",
    "The models we saw so far are fairly easy and work well in some scenarios, but sometimes it's not enough. What to do in these cases?\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"img/deeper_meme.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "### Deep Learning\n",
    "Deep learning is a subset of machine learning that involves training artificial neural networks to learn from data. Unlike traditional machine learning algorithms, which often rely on hand-crafted features and linear models, deep learning algorithms can automatically learn features and hierarchies of representations from raw data. This allows deep learning models to achieve state-of-the-art performance on a wide range of tasks in chemistry, like molecular property prediction, reaction prediction and retrosynthesis, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fff8f48-392d-4a61-9480-7751414bf029",
   "metadata": {},
   "source": [
    "#### Data: Let's go back to the [ESOL dataset](https://pubs.acs.org/doi/10.1021/ci034243x) from last week.\n",
    "We will use this so we can compare our results with the previous models. We'll reuse last week's code for  data loading and preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4bea2a-5762-4ebd-8fc0-af3b0b472d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# load dataset from the CSV file\n",
    "esol_df = pd.read_csv('data/esol.csv')\n",
    "\n",
    "# Get NumPy arrays from DataFrame for the input and target\n",
    "smiles = esol_df['smiles'].values\n",
    "y = esol_df['log solubility (mol/L)'].values\n",
    "\n",
    "# Here, we use molecular descriptors from RDKit, like molecular weight, number of valence electrons, maximum and minimum partial charge, etc.\n",
    "from deepchem.feat import RDKitDescriptors\n",
    "featurizer = RDKitDescriptors()\n",
    "features = featurizer.featurize(smiles)\n",
    "print(f\"Number of generated molecular descriptors: {features.shape[1]}\")\n",
    "\n",
    "# Drop the features containing invalid values\n",
    "import numpy as np\n",
    "features = features[:, ~np.isnan(features).any(axis=0)]\n",
    "print(f\"Number of molecular descriptors without invalid values: {features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eefda6-9512-4651-84b8-d39e68e97c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = features\n",
    "# training data size : test data size = 0.8 : 0.2\n",
    "# fixed seed using the random_state parameter, so it always has the same split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, random_state=0)\n",
    "\n",
    "# Create a validation set from the train set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, train_size=0.8, random_state=0)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# save original X\n",
    "X_train_ori = X_train\n",
    "X_test_ori = X_test\n",
    "# transform data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e20ea-8613-4580-976f-433b898efbf5",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Neural Networks are a type of machine learning model that is designed to simulate the behavior of the human brain.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"img/nn_image.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "\\\n",
    "They consist of layers of interconnected nodes, and each node applies a `linear function` to its inputs. Non-linear activation functions are used to introduce `non-linearity` into the model, allowing it to learn more complex patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef51c06-44a3-4581-b735-e08766108e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1749f9a1-4a86-413d-b4ce-7428856c5009",
   "metadata": {},
   "source": [
    "## Creating a deep learning model.\n",
    "\n",
    "Creating DL models is fairly easy nowadays, specially thanks to libraries like [Pytorch Lightning](https://pytorch-lightning.readthedocs.io/en/stable/index.html). They do most of the work for you, but they still alow you to have a lot of control over your models.\n",
    "\n",
    "To use Pytorch Lightning, we first need to know about **classes**.\n",
    "\n",
    "\n",
    "> Think of a class as a template or a set of instructions for creating objects with specific properties and behaviors. These objects are called instances of the class.\n",
    "\n",
    "\\\n",
    "For example, let's say you want to make a program to represent dogs.\n",
    "\n",
    "```python\n",
    "class Dog:\n",
    "    def __init__(self, name, color):\n",
    "        self.name = name\n",
    "        self.color = color\n",
    "        \n",
    "    def say_your_name(self):\n",
    "        print(f\"My name is {self.name}\")\n",
    "       \n",
    "```\n",
    "\n",
    "In this example, a dog has two attributes: `name` and `color`. It also has a method: `say_your_name`.\n",
    "\n",
    "Now we can create as many dogs as we want! For example\n",
    "\n",
    "```python\n",
    "lassie = Dog(name = \"Lassie\", color = \"White\")\n",
    "pluto = Dog(name = \"Pluto\", color = \"Yellow\")\n",
    "```\n",
    "\n",
    "And we can access their methods as follows:\n",
    "\n",
    "```python\n",
    "pluto.say_your_name()   # Output: \"My name is Pluto\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<font color=\"#4caf50\" size=4>\n",
    "Now let's define a NeuralNetwork class.\n",
    "</font>\n",
    "\n",
    "- What is each part? \n",
    "    - `__init__` is where we specify the model architecture, \n",
    "       There are loads of layers (model parts) you can use,\n",
    "       and it's all defined here.\n",
    "        \n",
    "    - `training step` is one of our model's methods. It updates the model paramters using an optimizer.\n",
    "    \n",
    "    - `configure_optimizers`, well, configures the optimizers 😅.\\\n",
    "       Here we define what optimizer to use, including learning rate.\n",
    "    \n",
    "    - `forward` specifices what the model should do when an input is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2457d73-1d02-437d-a5ef-a912fa862ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(pl.LightningModule):\n",
    "    def __init__(self, input_sz, hidden_sz, train_data, valid_data, test_data, batch_size=254, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.train_data = train_data\n",
    "        self.valid_data = valid_data\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Define all the components\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_sz, hidden_sz),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sz, hidden_sz),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sz, 1)\n",
    "        )\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Here we define the train loop.\n",
    "        x, y = batch\n",
    "        z = self.model(x)\n",
    "        loss = F.mse_loss(z, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Define validation step. At the end of every epoch, this will be executed\n",
    "        x, y = batch\n",
    "        z = self.model(x)\n",
    "        loss = F.mse_loss(z, y) ** 0.5  # report RMSE\n",
    "        self.log(\"validation loss\", loss)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # What to do in test\n",
    "        x, y = batch\n",
    "        z = self.model(x)\n",
    "        loss = F.mse_loss(z, y) ** 0.5  # report RMSE\n",
    "        self.log(\"test loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Here we configure the optimization algorithm.\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.lr\n",
    "        )\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Here we define what the NN does with its parts\n",
    "        return self.model(x).flatten()\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size = self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_data, batch_size = self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size = self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed0b379-723b-482c-95b3-9423261a171d",
   "metadata": {},
   "source": [
    "### Dataset class\n",
    "\n",
    "To use Lightning, we also need to create a `Dataset` class.\\\n",
    "It looks more complicated, but it actually allows a lot of flexibility in more complex scenarios! (so don't be daunted by this 😉)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab601a17-f42f-4ee8-8d59-8dbdb928dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ESOLDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        X_ = torch.as_tensor(self.X[idx].astype(np.float32))\n",
    "        y_ = torch.as_tensor(self.y[idx].astype(np.float32).reshape(-1))\n",
    "        \n",
    "        return X_, y_\n",
    "    \n",
    "train_data = ESOLDataset(X_train, y_train)\n",
    "valid_data = ESOLDataset(X_valid, y_valid)\n",
    "test_data = ESOLDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1f4682-5628-4284-815f-ebeab95d11de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This will ask you to login to your wandb account\n",
    "\n",
    "wandb.init(project=\"nn-solubility\",\n",
    "           config={\n",
    "               \"batch_size\": 500,\n",
    "               \"learning_rate\": 0.02,\n",
    "               \"hidden_size\": 254,\n",
    "               \"max_epochs\": 400\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321aa13d-a145-4bf1-bd8a-72d48170bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create an instance of our neural network.\n",
    "# Play around with the hyperparameters!\n",
    "nn_model = NeuralNetwork(\n",
    "    input_sz = X.shape[1],\n",
    "    hidden_sz = wandb.config[\"hidden_size\"],\n",
    "    train_data = train_data,\n",
    "    valid_data = valid_data,\n",
    "    test_data = test_data,\n",
    "    lr = wandb.config[\"learning_rate\"],\n",
    "    batch_size=wandb.config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "# Define trainer: How we want to train the model\n",
    "wandb_logger = WandbLogger()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs = wandb.config[\"max_epochs\"],\n",
    "    logger = wandb_logger\n",
    ")\n",
    "\n",
    "# Finally! Training a model :)\n",
    "trainer.fit(\n",
    "    model=nn_model,\n",
    ")\n",
    "\n",
    "# Now run test\n",
    "trainer.test()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4c1c9a-7867-4763-932f-9ac3da21f16e",
   "metadata": {},
   "source": [
    "# Exercise:\n",
    "\n",
    "Play with the hyperparameters, see what you get.\n",
    "\n",
    "You may play around with `hidden_sz`, `batch_sz`, `max_epochs`, `lr`,\\\n",
    "or even modify the architecture of our neural network i.e. change the number of layers, activation function, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a09570-53e0-4155-b4a1-c611692961f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# YOUR CODE #\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b8266-ec53-44a3-8051-99f40a358fb4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "One of the promises of deep learning algorithms is that they can learn to `automatically extract features from the raw data`.\\\n",
    "However, so far we have used the same featurization methods as we used for the more basic models.\n",
    "\n",
    "> Can our models directly take a molecule as input?\n",
    "\n",
    "# 2. Inductive biases\n",
    "\n",
    "**Inductive biases** are assumptions we make about the data, that help our models extract signal from it. These assumptions are encoded in the model's architecture.\n",
    "\n",
    "For instance, when we (humans) look at images, we think differently than when we read a book, or than when we analyze a molecule. **Processing all these different types of data requires different ways of interpretation, and thus different assumptions about the data**.\n",
    "\n",
    "When building models, we attempt to encode these inductive biases in our model's architecture so they know how to read and process the data.\n",
    "\n",
    "### Introducing Graph Neural Networks\n",
    "A natural way of representing molecules is as graphs. A graph is a collection of nodes (atoms) and edges (bonds). \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"img/Chloroquine-2D-molecular-graph.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Each node has some list of basic properties (think of a carbon atom, it has 6 electrons, it's electronic configuration is $1s^22s^22p^2$, etc.); however some properties change depending on the atomic environment (e.g. clearly a carbonyl C acts very differenly from a carbene). \n",
    "\n",
    "In the end, this is what we assume from the data:\n",
    "\n",
    "> Molecules are formed by atoms connected by bonds, and each atom is influenced mostly by its closest neighbors.\\\n",
    "> Molecular properties are determined solely by the molecular graph.\n",
    "\n",
    "This is what **we assume** and thus what **we tell our model**. The specific details of how to calculate the solubility of a molecule (or any other property), that's exactly what the model will try to learn from the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95947e57",
   "metadata": {},
   "source": [
    "# 2. Graph neural network in chemistry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de6865",
   "metadata": {},
   "source": [
    "## 2.1 Graph representation\n",
    "\n",
    "In [graph theory](https://en.wikipedia.org/wiki/Graph_theory), a graph $G=(V,E)$ is defined by a set of **nodes** (also called **vertices**) $V$ and a set of **edges** (also called **links**) $E$ between these vertices. More specifically:\n",
    "\n",
    "- $V = \\{ v_1, \\: ..., \\: v_n \\}$, a set of nodes;\n",
    "- $E \\subseteq \\{ (i,j) \\: | \\: i,j \\in V,  \\: i \\neq j \\}$, a set of edges representing connections between nodes.\n",
    "\n",
    "If the edges of a graph have directions, the graph is called a directed graph, otherwise it is called an undirected graph.\n",
    "\n",
    "<center width=\"100%\" style=\"padding:10px\"><img src=\"img/graphs.png\" width=\"250px\"></center>\n",
    "\n",
    "In many cases we also have attribute or feature information associated with a graph:\n",
    "- node features: $\\mathbf{X} = [..., \\: x_i, \\: ...]^T \\in \\mathbb{R}^{|V| \\times m}$, and $x_i \\in \\mathbb{R}^m$ denotes the feature of node $i$;\n",
    "- edge features: $\\mathbf{L} = [..., \\: l_{i,j}, \\: ...]^T \\in \\mathbb{R}^{|E| \\times r}$, and $l_{i,j} \\in \\mathbb{R}^r$ denotes the feature of the edge between node $i$ and node $j$;\n",
    "- graph features: $\\mathbf{G} = (..., \\: g_i, \\: ...) \\in \\mathbb{R}^s$, and $g_i$ is the feature (or label) $i$ of the graph, which is usually the prediction target.\n",
    "\n",
    "For instance, let's look at the following undirected graph with node features:\n",
    "\n",
    "<center width=\"100%\" style=\"padding:10px\"><img src=\"img/graph_example.svg\" width=\"250px\"></center>\n",
    "\n",
    "This graph has 4 nodes and 4 edges. The nodes are $V=\\{1,2,3,4\\}$, and edges $E=\\{(1,2), (2,3), (2,4), (3,4)\\}$. Note that for simplicity, we don't add mirrored pairs like $(2,1)$. And we have the following node features:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "    0 & 1 & 2\\\\\n",
    "    1 & 0 & 1\\\\\n",
    "    1 & 1 & 0\\\\\n",
    "    3 & 1 & 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The **adjacency matrix** $A$ is a square matrix whose elements indicate whether pairs of nodes are adjacent, i.e. connected, or not. In the simplest case, $A_{ij}$ is 1 if there is a connection from node $i$ to $j$, and otherwise 0. For an undirected graph, keep in mind that $A$ is a symmetric matrix ($A_{ij}=A_{ji}$). For the example graph above, we have the following adjacency matrix:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "    0 & 1 & 0 & 0\\\\\n",
    "    1 & 0 & 1 & 1\\\\\n",
    "    0 & 1 & 0 & 1\\\\\n",
    "    0 & 1 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0465b86",
   "metadata": {},
   "source": [
    "### Molecular graph\n",
    "A [molecular graph](https://en.wikipedia.org/wiki/Molecular_graph) is a labeled graph whose nodes correspond to the atoms of the compound and edges correspond to chemical bonds. It also has node features (**atom features**), edge features (**bond features**) and graph labels (chemical properties of a molecule). Next, we demonstrate a simple example of building a molecular graph (undirected). In this example, we do not consider hydrogen atoms as nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bd3c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import MolFromSmiles\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "IPythonConsole.ipython_useSVG = True  # < use SVGs instead of PNGs\n",
    "IPythonConsole.drawOptions.addAtomIndices = True  # adding indices for atoms\n",
    "IPythonConsole.drawOptions.addBondIndices = False  # not adding indices for bonds\n",
    "IPythonConsole.molSize = 200, 200\n",
    "\n",
    "# N,N-dimethylformamide (DMF)\n",
    "dmf_smiles = 'CN(C)C=O'\n",
    "mol = MolFromSmiles(dmf_smiles)\n",
    "# show molecular graph of DMF, atom indices = node indices\n",
    "mol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcba6a2",
   "metadata": {},
   "source": [
    "### Atom feature\n",
    "\n",
    "| feature | description |\n",
    "| ---- | ----  |\n",
    "| atom type  | atomic number |\n",
    "| degree  | number of directly-bonded neighbor atoms, including H atoms |\n",
    "| formal charge | integer electronic charge assigned to atom |\n",
    "| hybridization | sp, sp2, sp3, sp3d, or sp3d2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cab5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATOM_FEATURES = {\n",
    "    'atom_type' : [1, 6, 7, 8, 9],  # elements: H, C, N, O, F\n",
    "    'degree' : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 'misc'],\n",
    "    'formal_charge' : [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 'misc'],\n",
    "    'hybridization' : [\n",
    "        'SP', 'SP2', 'SP3', 'SP3D', 'SP3D2', 'misc'\n",
    "        ],\n",
    "}\n",
    "\n",
    "def get_atom_fv(atom):\n",
    "    \"\"\"\n",
    "    Converts rdkit atom object to feature list of indices\n",
    "    :param atom: rdkit atom object\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    atom_fv = [\n",
    "        ATOM_FEATURES['atom_type'].index(atom.GetAtomicNum()),\n",
    "        ATOM_FEATURES['degree'].index(atom.GetTotalDegree()),\n",
    "        ATOM_FEATURES['formal_charge'].index(atom.GetFormalCharge()),\n",
    "        ATOM_FEATURES['hybridization'].index(str(atom.GetHybridization())),\n",
    "    ]\n",
    "    return atom_fv\n",
    "\n",
    "atom_fvs = [get_atom_fv(atom) for atom in mol.GetAtoms()]\n",
    "atom_fvs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a4906b",
   "metadata": {},
   "source": [
    "### Bond feature\n",
    "\n",
    "| feature | description |\n",
    "| ---- | ----  |\n",
    "| bond type  | single, double, triple, or aromatic |\n",
    "| stereo | none, any, E/Z or cis/trans |\n",
    "| conjugated  | whether the bond is conjugated |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03427568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show indices of bonds\n",
    "IPythonConsole.drawOptions.addAtomIndices = False  # not adding indices for atoms\n",
    "IPythonConsole.drawOptions.addBondIndices = True  # adding indices for bonds\n",
    "mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOND_FEATURES = {\n",
    "    'bond_type' : [\n",
    "        'SINGLE',\n",
    "        'DOUBLE',\n",
    "        'TRIPLE',\n",
    "        'AROMATIC',\n",
    "        'misc'\n",
    "    ],\n",
    "    'stereo': [\n",
    "        'STEREONONE',\n",
    "        'STEREOZ',\n",
    "        'STEREOE',\n",
    "        'STEREOCIS',\n",
    "        'STEREOTRANS',\n",
    "        'STEREOANY',\n",
    "    ], \n",
    "    'conjugated': [False, True],\n",
    "}\n",
    "\n",
    "def get_bond_fv(bond):\n",
    "    \"\"\"\n",
    "    Converts rdkit bond object to feature list of indices\n",
    "    :param bond: rdkit bond object\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    bond_fv = [\n",
    "        BOND_FEATURES['bond_type'].index(str(bond.GetBondType())),\n",
    "        BOND_FEATURES['stereo'].index(str(bond.GetStereo())),\n",
    "        BOND_FEATURES['conjugated'].index(bond.GetIsConjugated()),\n",
    "    ]\n",
    "    return bond_fv\n",
    "\n",
    "bond_fvs = [get_bond_fv(bond) for bond in mol.GetBonds()]\n",
    "bond_fvs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b4193",
   "metadata": {},
   "source": [
    "### Edge index\n",
    "In many cases, a list of paired node indices are used to describe edges rather than adjacency matrix. Here we use paired node indices (`edge_index`) with shape (2, num_edges) to define the edges in a graph.\n",
    "\n",
    "$$\n",
    "\\mathbf{E} = \\begin{bmatrix}\n",
    "    ..., & i, & ..., & j, & ... \\\\\n",
    "    ..., & j, & ..., & i, & ...\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Like, there has an edge between node $i$ and node $j$ (undirected graph).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4291b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index0, edge_index1 = [], []\n",
    "\n",
    "for bond in mol.GetBonds():\n",
    "    i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "    edge_index0 += [i, j]\n",
    "    edge_index1 += [j, i]\n",
    "\n",
    "edge_index = [edge_index0, edge_index1]\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbf2dbb",
   "metadata": {},
   "source": [
    "### Molecular graph data\n",
    "\n",
    "We set the density of DMF(0.944 $g/cm^3$) as the graph feature (label). Here we use [Data](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Data.html#torch_geometric.data.Data) class in `PyG` to create a graph data for DMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# convert our data to tensors, which are used for model training\n",
    "x = torch.tensor(atom_fvs, dtype=torch.float)\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "edge_attr = torch.tensor(bond_fvs, dtype=torch.float)\n",
    "y = torch.tensor([0.944], dtype=torch.float)\n",
    "\n",
    "dmf_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "dmf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b4e523",
   "metadata": {},
   "source": [
    "## 2.2 Graph Neural Network\n",
    "\n",
    "A [graph neural network (GNN)](https://en.wikipedia.org/wiki/Graph_neural_network) is a class of artificial neural networks for processing data that can be represented as graphs. GNNs rely on [message passing methods](https://arxiv.org/abs/1704.01212), which means that nodes exchange information with the neighbors, and send \"messages\" to each other. Generally, GNNs operate in two phases: a **message passing** phase, which transmits information across the molecule to build a neural representation of the molecule, and a **readout** phase, which uses the final representation of the molecule to make predictions about the properties of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0dddb7",
   "metadata": {},
   "source": [
    "### Message passing\n",
    "\n",
    "Before looking at the math, we can try to visually understand how message passing works. The first step is that each node creates a `feature vector` that represents the `message` it wants to send to all its neighbors. In the second step, the messages are sent to the neighbors, so that a node receives one message per adjacent node. As shown in the figure below, after a message passing step, `node 1` can get the message from `node 2`, and `node 2` can get messages from `node 1`, `node 3` and `node 4`. The third step is that each node will aggregate all messages from neighbors and get a `message vector`. Then, the fourth step is that each node updates its `feature vector` based on its `message vector` and previous `feature vector`.\n",
    "\n",
    "<center width=\"100%\" style=\"padding:10px\"><img src=\"img/graph_message_passing.svg\" width=\"700px\"></center>\n",
    "\n",
    "Moreover, with the iteration of message passing, each node can obtain the feature vectors of more distant nodes and not limited to neighbors. As shown in the figure below, node `A` can get informations from node `E` and node `F` in the interation 2, which are not the neighbors of node `A`.  Node `C`, the neighbor of node `A`, can obtain the information of nodes `E` and `F` in the iteration 1, so node `A` can obtain the information of nodes `E` and `F` in the iteration 2.\n",
    "\n",
    "<center width=\"100%\" style=\"padding:10px\"><img src=\"img/messages.svg\" width=\"700px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1a10fa",
   "metadata": {},
   "source": [
    "### Readout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de73f99",
   "metadata": {},
   "source": [
    "Here, we will define a GNN model using message passing neural network (MPNN) according to paper [\"Neural Message Passing for Quantum Chemistry\"](https://arxiv.org/abs/1704.01212). We just use [NNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.NNConv.html#torch_geometric.nn.conv.NNConv) class to create message passing layers of our models. The [torch_geometric.nn](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html) module of PyG contains many different types of layers for message passing and readout, which can help us define GNN models more conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import GRU\n",
    "\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import NNConv, MLP, global_add_pool\n",
    "\n",
    "\n",
    "class MPNN(pl.LightningModule):\n",
    "    def __init__(self, hidden_dim, out_dim,\n",
    "                 train_data, valid_data, test_data,\n",
    "                 std, batch_size=32, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.std = std  # std of data's target\n",
    "        self.train_data = train_data\n",
    "        self.valid_data = valid_data\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        # Initial layers\n",
    "        self.atom_emb = AtomEncoder(emb_dim=hidden_dim)\n",
    "        self.bond_emb = BondEncoder(emb_dim=hidden_dim)\n",
    "        # Message passing layers\n",
    "        nn = MLP([hidden_dim, hidden_dim*2, hidden_dim*hidden_dim])\n",
    "        self.conv = NNConv(hidden_dim, hidden_dim, nn, aggr='mean')\n",
    "        self.gru = GRU(hidden_dim, hidden_dim)\n",
    "        # Readout layers\n",
    "        self.mlp = MLP([hidden_dim, int(hidden_dim/2), out_dim])\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "\n",
    "        # Initialization\n",
    "        x = self.atom_emb(data.x)\n",
    "        h = x.unsqueeze(0)\n",
    "        edge_attr = self.bond_emb(data.edge_attr)\n",
    "        \n",
    "        # Message passing\n",
    "        for i in range(3):\n",
    "            m = F.relu(self.conv(x, data.edge_index, edge_attr))\n",
    "            x, h = self.gru(m.unsqueeze(0), h)\n",
    "            x = x.squeeze(0)\n",
    "\n",
    "        # Readout\n",
    "        x = global_add_pool(x, data.batch)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x.view(-1)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Here we define the train loop.\n",
    "        out = self.forward(batch, mode=\"train\")\n",
    "        loss = F.mse_loss(out, batch.y)\n",
    "        self.log(\"Train loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Define validation step. At the end of every epoch, this will be executed\n",
    "        out = self.forward(batch, mode=\"valid\")\n",
    "        loss = F.mse_loss(out * self.std, batch.y * self.std)  # report MSE\n",
    "        self.log(\"Valid MSE\", loss)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # What to do in test\n",
    "        out = self.forward(batch, mode=\"test\")\n",
    "        loss = F.mse_loss(out * self.std, batch.y * self.std)  # report MSE\n",
    "        self.log(\"Test MSE\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Here we configure the optimization algorithm.\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.lr\n",
    "        )\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_data, batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04593bf6",
   "metadata": {},
   "source": [
    "Here, we can use [InMemoryDataset]() class in PyG to create the graph dataset of ESOL conveniently. You can also browse its [tutorial](https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html) and [pre-defined dataset](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html) to learn about how to create graph datasets quickly by PyG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7eeec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    "    InMemoryDataset,\n",
    "    download_url,\n",
    ")\n",
    "from ogb.utils import smiles2graph\n",
    "\n",
    "\n",
    "class ESOLGData(InMemoryDataset):\n",
    "    \"\"\"The ESOL graph dataset using PyG\n",
    "    \"\"\"\n",
    "    # ESOL dataset download link\n",
    "    raw_url = 'https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/delaney-processed.csv'\n",
    "\n",
    "    def __init__(self, root, transform=None):\n",
    "        super().__init__(root, transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['delaney-processed.csv']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        print('Downloading ESOL dataset...')\n",
    "        file_path = download_url(self.raw_url, self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        # load raw data from a csv file\n",
    "        df = pd.read_csv(self.raw_paths[0])\n",
    "        smiles = df['smiles'].values.tolist()\n",
    "        target = df['measured log solubility in mols per litre'].values.tolist()\n",
    "\n",
    "        # Convert SMILES into graph data\n",
    "        print('Converting SMILES strings into graphs...')\n",
    "        data_list = []\n",
    "        for i, smi in enumerate(tqdm(smiles)):\n",
    "\n",
    "            # get graph data from SMILES\n",
    "            graph = smiles2graph(smi)\n",
    "\n",
    "            # convert to tensor and pyg data\n",
    "            x = torch.tensor(graph['node_feat'], dtype=torch.long)\n",
    "            edge_index = torch.tensor(graph['edge_index'], dtype=torch.long)\n",
    "            edge_attr = torch.tensor(graph['edge_feat'], dtype=torch.long)\n",
    "            y = torch.tensor([target[i]], dtype=torch.float)\n",
    "            data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "            data_list.append(data)\n",
    "\n",
    "        # save data\n",
    "        torch.save(self.collate(data_list), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f352ca3",
   "metadata": {},
   "source": [
    "Create, normalize and split ESOL graph dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb4e4f8-86b4-4edd-998d-897d5eb02d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.splits import RandomSplitter\n",
    "\n",
    "# create dataset\n",
    "dataset = ESOLGData('./esol_pyg').shuffle()\n",
    "\n",
    "# Normalize target to mean = 0 and std = 1.\n",
    "mean = dataset.data.y.mean()\n",
    "std = dataset.data.y.std()\n",
    "dataset.data.y = (dataset.data.y - mean) / std\n",
    "mean, std = mean.item(), std.item()\n",
    "\n",
    "# split data\n",
    "splitter = RandomSplitter()\n",
    "train_idx, valid_idx, test_idx = splitter.split(dataset, frac_train=0.7, frac_valid=0.1, frac_test=0.2, seed=0)\n",
    "train_dataset = dataset[train_idx]\n",
    "valid_dataset = dataset[valid_idx]\n",
    "test_dataset = dataset[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a83e75-11ec-435c-815a-ebb1a9ccdb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will ask you to login to your wandb account\n",
    "\n",
    "wandb.init(project=\"gnn-solubility\",\n",
    "           config={\n",
    "               \"batch_size\": 32,\n",
    "               \"learning_rate\": 0.001,\n",
    "               \"hidden_size\": 64,\n",
    "               \"max_epochs\": 60\n",
    "           })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee98743",
   "metadata": {},
   "source": [
    "Train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50f7cb0-a057-4521-a047-66b4d51fd27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create an instance of our GNN.\n",
    "# Play around with the hyperparameters!\n",
    "gnn_model = MPNN(\n",
    "    hidden_dim=wandb.config[\"hidden_size\"],\n",
    "    out_dim=1,\n",
    "    std=std,\n",
    "    train_data=train_dataset,\n",
    "    valid_data=valid_dataset,\n",
    "    test_data=test_dataset,\n",
    "    lr=wandb.config[\"learning_rate\"],\n",
    "    batch_size=wandb.config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "# Define trainer: How we want to train the model\n",
    "wandb_logger = WandbLogger()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs = wandb.config[\"max_epochs\"],\n",
    "    logger = wandb_logger\n",
    ")\n",
    "\n",
    "# Finally! Training a model :)\n",
    "trainer.fit(\n",
    "    model=gnn_model,\n",
    ")\n",
    "\n",
    "# Now run test\n",
    "trainer.test(ckpt_path=\"best\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca2cd0-b13a-4e1c-a500-7e4fc0b4842f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Is DL always better?\n",
    "\n",
    "Deep learning models are not always the best choice for every problem. One of the challenges in deep learning is that the models can be highly sensitive to small changes in the input, which can result in poor performance on certain types of data. One example of this is the concept of activity cliffs in the chemical space. Activity cliffs are regions where small changes in the structure of a molecule result in large changes in its activity. Deep learning models may not always be the best choice for predicting these activity cliffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30845819-7de2-407c-a368-ad9f62020428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1b1e114f4dae097b9e32029c5d22d73dc21a5dd723446d46774bd2adced9390"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
