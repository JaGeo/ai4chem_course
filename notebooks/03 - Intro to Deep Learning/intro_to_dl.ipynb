{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5e768a-9b68-48c6-9af0-9281a9762d6f",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/schwallergroup/ai4chem_course/blob/scikit_learn/notebooks/02%20-%20Supervised%20Learning/training_and_evaluating_ml_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bae14-53f5-4adf-a28c-d8658dd8cd85",
   "metadata": {},
   "source": [
    "# Week 3 tutorial - AI 4 Chemistry\n",
    "\n",
    "## Table of content\n",
    "\n",
    "1. Supervised deep learning.\n",
    "2. Inductive biases.\n",
    "3. Training neural networks.\n",
    "4. Model selection and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9526aa9b-ed9f-4faa-add7-40139fad09c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/andres/anaconda3/lib/python3.9/site-packages (1.21.5)\n",
      "Requirement already satisfied: scipy in /home/andres/anaconda3/lib/python3.9/site-packages (1.8.1)\n"
     ]
    }
   ],
   "source": [
    "# Install all libraries\n",
    "!pip install numpy scipy\n",
    "\n",
    "# Download all data\n",
    "!mkdir data/\n",
    "!wget https://raw.githubusercontent.com/schwallergroup/ai4chem_course/scikit_learn/notebooks/02%20-%20Supervised%20Learning/data/esol.csv -O data/esol.csv\n",
    "!wget https://raw.githubusercontent.com/schwallergroup/ai4chem_course/scikit_learn/notebooks/02%20-%20Supervised%20Learning/data/toxcast_data.csv -O data/toxcast_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5e52b4-30d8-4940-9d54-e71e5ece96ce",
   "metadata": {},
   "source": [
    "# 1. Supervised Deep Learning\n",
    "\n",
    "From last session we should already be familiar with supervised learning: is a type of machine learning that involves training a model on a labeled dataset to learn the relationships between input and output data.\n",
    "\n",
    "The models we saw so far are fairly easy and work well in some scenarios, but sometimes it's not enough. What to do in these cases?\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"img/deeper_meme.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "### Deep Learning\n",
    "Deep learning is a subset of machine learning that involves training artificial neural networks to learn from data. Unlike traditional machine learning algorithms, which often rely on hand-crafted features and linear models, deep learning algorithms can automatically learn features and hierarchies of representations from raw data. This allows deep learning models to achieve state-of-the-art performance on a wide range of tasks in chemistry, like molecular property prediction, reaction prediction and retrosynthesis, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fff8f48-392d-4a61-9480-7751414bf029",
   "metadata": {},
   "source": [
    "#### Data: Let's go back to the [ESOL dataset](https://pubs.acs.org/doi/10.1021/ci034243x) from last week.\n",
    "We will use this so we can compare our results with the previous models. We'll reuse last week's code for  data loading and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b4bea2a-5762-4ebd-8fc0-af3b0b472d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/andres/anaconda3/envs/ai4chem/lib/python3.8/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of generated molecular descriptors: 208\n",
      "Number of molecular descriptors without invalid values: 208\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# load dataset from the CSV file\n",
    "esol_df = pd.read_csv('data/esol.csv')\n",
    "\n",
    "# Get NumPy arrays from DataFrame for the input and target\n",
    "smiles = esol_df['smiles'].values\n",
    "y = esol_df['log solubility (mol/L)'].values\n",
    "\n",
    "# Here, we use molecular descriptors from RDKit, like molecular weight, number of valence electrons, maximum and minimum partial charge, etc.\n",
    "from deepchem.feat import RDKitDescriptors\n",
    "featurizer = RDKitDescriptors()\n",
    "features = featurizer.featurize(smiles)\n",
    "print(f\"Number of generated molecular descriptors: {features.shape[1]}\")\n",
    "\n",
    "# Drop the features containing invalid values\n",
    "import numpy as np\n",
    "features = features[:, ~np.isnan(features).any(axis=0)]\n",
    "print(f\"Number of molecular descriptors without invalid values: {features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1eefda6-9512-4651-84b8-d39e68e97c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = features\n",
    "# training data size : test data size = 0.8 : 0.2\n",
    "# fixed seed using the random_state parameter, so it always has the same split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, random_state=0)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# save original X\n",
    "X_train_ori = X_train\n",
    "X_test_ori = X_test\n",
    "# transform data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e20ea-8613-4580-976f-433b898efbf5",
   "metadata": {},
   "source": [
    "## 1.1 Neural Networks\n",
    "\n",
    "Neural Networks are a type of machine learning model that is designed to simulate the behavior of the human brain.\\\n",
    "They consist of layers of interconnected nodes, and each node applies a `linear function` to its inputs. Non-linear activation functions are used to introduce `non-linearity` into the model, allowing it to learn more complex patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ef51c06-44a3-4581-b735-e08766108e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1749f9a1-4a86-413d-b4ce-7428856c5009",
   "metadata": {},
   "source": [
    "### 1.2. Creating a deep learning model. TODO\n",
    "\n",
    "#### 1.2.1. Classes: We can define a class to represent abstract objects that have properties and that do things.\n",
    "\n",
    "We can define a `Dog` class like this:\n",
    "\n",
    "```python\n",
    "class Dog:\n",
    "    def __init__(self):\n",
    "        self.color = \"brown\"\n",
    "        self.weight = 43\n",
    "        \n",
    "    def bark(self):\n",
    "        print(\"Woof!\")\n",
    "```\n",
    "\n",
    "In this example, a dog has two properties: `color` and `weight`. You can define more if you want a more accurate representation of a dog :)\\\n",
    "Our dog can also `bark`, and this is a __thing it can do__, or a `method`. Again, you can define more methods for your class.\n",
    "\n",
    "\n",
    "<font color=\"#4caf50\" size=4>\n",
    "Now let's define a NeuralNetwork class.\n",
    "</font>\n",
    "\n",
    "- What is each part? \n",
    "    - `__init__` is where we specify the model architecture, \n",
    "        \n",
    "        There are loads of layers (model parts) you can use,\n",
    "        and it's all defined here.\n",
    "        \n",
    "    - `training step` is one of our model's methods. It updates the model paramters using an optimizer.\n",
    "    \n",
    "    - `configure_optimizers`, well, configures the optimizers ðŸ˜…. Here we define what optimizer to use, including learning rate.\n",
    "    \n",
    "    - `forward` specifices what the model should do when an input is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f381b5ac-a2df-4cc9-8ac4-36de8d55cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(pl.LightningModule):\n",
    "    def __init__(self, input_sz, hidden_sz, output_sz, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Define all the components\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_sz, hidden_sz),\n",
    "            nn.ReLU(),\n",
    "            #nn.Linear(hidden_sz, hidden_sz),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(hidden_sz, output_sz)\n",
    "        )\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Here we define the train loop.\n",
    "        x, y = batch\n",
    "        z = self.layers(x)\n",
    "        loss = F.mse_loss(z, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Here we configure the optimization algorithm.\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.lr\n",
    "        )\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Here we define what the NN does with its parts\n",
    "        return self.layers(x).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed0b379-723b-482c-95b3-9423261a171d",
   "metadata": {},
   "source": [
    "### Dataset class\n",
    "\n",
    "To use Lightning, we also need to create a `Dataset` class.\\\n",
    "It looks more complicated, but it actually allows a lot of flexibility in more complex scenarios! (so don't be daunted by this ðŸ˜‰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab601a17-f42f-4ee8-8d59-8dbdb928dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ESOLDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        X_ = torch.as_tensor(self.X[idx].astype(np.float32))\n",
    "        y_ = torch.as_tensor(self.y[idx].astype(np.float32).reshape(-1))\n",
    "        \n",
    "        return X_, y_\n",
    "    \n",
    "train_data = ESOLDataset(X_train, y_train)\n",
    "test_data = ESOLDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e1f4682-5628-4284-815f-ebeab95d11de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | Sequential | 53.3 K\n",
      "--------------------------------------\n",
      "53.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "53.3 K    Total params\n",
      "0.213     Total estimated model params size (MB)\n",
      "/home/andres/anaconda3/envs/ai4chem/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/andres/anaconda3/envs/ai4chem/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32114d36b7d4a239bb7870d4a2152d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=254)\n",
    "nn_model = NeuralNetwork(208, 254, 1, lr=1e-2)\n",
    "\n",
    "# Define trainer: How we want to train the model\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100\n",
    ")\n",
    "\n",
    "# Finally! Training a model :)\n",
    "trainer.fit(\n",
    "    model=nn_model,\n",
    "    train_dataloaders=train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d98dff10-b455-4db2-adc2-6c0ac9e9ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def mse_nn(model, test_data):\n",
    "    X_tens, y_tens = test_data.__getitem__(range(len(test_data)))\n",
    "    y_pred = nn_model(X_tens).detach()\n",
    "    y_pred = y_pred.numpy()\n",
    "    y_tens = y_tens.numpy()\n",
    "    \n",
    "    return mean_squared_error(y_tens, y_pred)\n",
    "\n",
    "def test_model(model, train_data, test_data):\n",
    "    \"\"\"\n",
    "    Function that tests a model.\n",
    "    Inputs: model, train_data, test_data\n",
    "    \"\"\"\n",
    "    # Calculate RMSE\n",
    "    mse_train = mse_nn(nn_model, train_data) ** 0.5\n",
    "    mse_test = mse_nn(nn_model, test_data) ** 0.5\n",
    "    print(f\"RMSE on train set: {mse_train:.3f}, and test set: {mse_test:.3f}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e2bf533-fb17-44b4-9d20-19304b9fe377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on train set: 0.416, and test set: 0.692.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model(nn_model, train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4c1c9a-7867-4763-932f-9ac3da21f16e",
   "metadata": {},
   "source": [
    "# Exercise:\n",
    "\n",
    "Play with the hyperparameters, see what you get.\n",
    "\n",
    "You may play around with `hidden_sz`, `batch_sz`, `max_epochs`, `lr`,\\\n",
    "or even modify the architecture of our neural network i.e. change the number of layers, activation function, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b93892d-bde3-4723-a766-9495d27a1d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6feba-e59e-47bc-9ef5-6515f4090bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb4e4f8-86b4-4edd-998d-897d5eb02d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a83e75-11ec-435c-815a-ebb1a9ccdb28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50f7cb0-a057-4521-a047-66b4d51fd27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4eca2cd0-b13a-4e1c-a500-7e4fc0b4842f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inductive biases\n",
    "\n",
    "Inductive biases are assumptions that are built into the design of a machine learning model. These biases can help the model learn more quickly and accurately, but they can also make it less flexible and adaptable to new situations. Inductive biases are a trade-off between accuracy and flexibility.\n",
    "\n",
    "\n",
    "### How to train\n",
    "\n",
    "Training a neural network involves selecting an appropriate architecture, initializing the weights of the model, and then iteratively adjusting the weights of the model to minimize the error between the predicted output and the actual output. This is typically done using an optimization algorithm such as gradient descent.\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "Gradient descent is an optimization algorithm that is used to train neural networks. It works by iteratively adjusting the weights of the model to minimize the error between the predicted output and the actual output. The goal is to find the weights that minimize the loss function.\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "Backpropagation is a method for computing the gradients of the loss function with respect to the weights of the model. It is used to update the weights during training. Backpropagation is an efficient way to compute the gradients of the loss function with respect to the weights of the model, and it enables the use of gradient-based optimization algorithms such as gradient descent.\n",
    "\n",
    "### Loss functions\n",
    "\n",
    "Loss functions are used to measure the difference between the predicted output of the model and the actual output. They are used to guide the optimization algorithm during training. Commonly used loss functions include mean squared error, cross-entropy loss, and binary cross-entropy loss.\n",
    "\n",
    "### Is DL always better?\n",
    "\n",
    "Deep learning models are not always the best choice for every problem. One of the challenges in deep learning is that the models can be highly sensitive to small changes in the input, which can result in poor performance on certain types of data. One example of this is the concept of activity cliffs in the chemical space. Activity cliffs are regions where small changes in the structure of a molecule result in large changes in its activity. Deep learning models may not always be the best choice for predicting these activity cliffs.\n",
    "\n",
    "### LSTM from smiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30845819-7de2-407c-a368-ad9f62020428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4chem",
   "language": "python",
   "name": "ai4chem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
