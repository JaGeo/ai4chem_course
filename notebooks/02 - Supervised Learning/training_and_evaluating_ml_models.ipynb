{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b6d7b6-698e-4ea2-b8e9-bad6f279fcfe",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/schwallergroup/ai4chem_course/blob/scikit_learn/notebooks/02%20-%20Supervised%20Learning/training_and_evaluating_ml_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15d3c35",
   "metadata": {},
   "source": [
    "# 0. Software\n",
    "### Scikit-learn\n",
    "Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities. It is widely used in industry and academia, and a wealth of tutorials and code snippets are available online.\n",
    "We will learn to use scikit-learn to do machine learning work. You can also browse the scikit-learn [user guide](https://scikit-learn.org/stable/user_guide.html) and [tutorials](https://scikit-learn.org/stable/tutorial/index.html) for additional details.\n",
    "### Essential Libraries and Tools \n",
    "Scikit-learn depends on two other Python packages, NumPy and SciPy. For plotting and interactive development, you should also install matplotlib, IPython, and the Jupyter Notebook.\n",
    "- **NumPy** is one of the fundamental packages for scientific computing in Python. It contains functionality for multidimensional arrays, high-level mathematical functions such as linear algebra operations and the Fourier transform, and pseudorandom number generators. In scikit-learn, the NumPy array is the fundamental data structure. scikit-learn takes in data in the form of NumPy arrays. Any data you’re using will have to be converted to a NumPy array.\n",
    "- **SciPy** is a collection of functions for scientific computing in Python. It provides, among other functionality, advanced linear algebra routines, mathematical function optimization, signal processing, special mathematical functions, and statistical distributions. scikit-learn draws from SciPy’s collection of functions for implementing its algorithms.\n",
    "- **Matplotlib** is the primary scientific plotting library in Python. It provides functions for making publication-quality visualizations such as line charts, histograms, scatter plots, and so on.\n",
    "- **Pandas** Python library for data wrangling and analysis. It is built around a data structure called the DataFrame that is similar to an Excel spreadsheet. It can ingest from a great variety of file formats and databases, like SQL, Excel files, and comma-separated values (CSV) files.\n",
    "\n",
    "### XGBoost\n",
    "XGBoost (eXtreme Gradient Boosting) is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. You can also browse the [XGBoost Documentation](https://xgboost.readthedocs.io/en/stable/) for additional details.\n",
    "\n",
    "### DeepChem\n",
    "DeepChem is a high quality open-source toolchain that democratizes the use of deep-learning in chemistry, biology and materials science. It also provides various tools for dataset loader, splitters, molecular featurization, model construction and hyperparameter tuning. You can also browse the [DeepChem Ducumentation](https://deepchem.readthedocs.io/en/latest/) for additional details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fbf541",
   "metadata": {},
   "source": [
    "We will first install the required libraries. We also need `RDKit` library to process and analyze molecules, like calculating molecular descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4688b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy scipy matplotlib scikit-learn pandas rdkit xgboost deepchem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ae06f",
   "metadata": {},
   "source": [
    "# 1. Introduction to Machine learning\n",
    "Machine learning (ML) is a field of inquiry devoted to understanding and building methods that \"learn\" – that is, methods that leverage data to improve performance on some set of tasks. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://s3.ap-southeast-1.amazonaws.com/files-scs-prod/public%2Fimages%2F1605842918803-AI+vs+ML+vs+DL.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Machine learning approaches are traditionally divided into three broad categories, depending on the nature of the \"signal\" or \"feedback\" available to the learning system:\n",
    "- **Supervised learning**: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs.\n",
    "- **Unsupervised learning**: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).\n",
    "- **Reinforcement learning**: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximize.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://starship-knowledge.com/wp-content/uploads/2021/01/unsupervised_supervised_reinforcement.jpeg\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba2fc2",
   "metadata": {},
   "source": [
    "# 2. Supervised learning\n",
    "Two major types of supervised machine learning problems:\n",
    "- **Classification** task is to predict a class label, which is a choice from a predefined list of possibilities. For example, to determine whether the photo is a dog, a cat or a rabbit.\n",
    "- **Regression** task is to predict a continuous number, or a floating-point number in programming terms (or real number in mathematical terms), like predicting a person’s annual income from their education, their age, and where they live.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*xs6Jr4iAPvoqszF9JgDWOA.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "## Common algorithms\n",
    "- k-Nearest Neighbors (k-NN)\n",
    "- Linear Models\n",
    "- Support Vector Machines\n",
    "- Decision Trees\n",
    "- Ensembles of Decision Trees\n",
    "  - Random forests\n",
    "  - Gradient boosting machines\n",
    "\n",
    "We can use `scikit-learn` to create ML models of different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3) # instantiate the model and set the number of neighbors to consider to 3\n",
    "\n",
    "# k-NN regressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=3) # instantiate the model and set the number of neighbors to consider to 3\n",
    "\n",
    "# linear regressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# decision tree classifier & regressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "\n",
    "# random forest classifier & regressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "ranf_clf = RandomForestClassifier(n_estimators=10)  # using 10 trees\n",
    "ranf_reg = RandomForestRegressor(n_estimators=10)  # using 10 trees\n",
    "\n",
    "# XGBoost classifier & regressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "bst_clf = XGBClassifier(n_estimators=10)  # using 10 trees\n",
    "bst_reg = XGBRegressor(n_estimators=10)  # using 10 trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be81ecfe",
   "metadata": {},
   "source": [
    "## Model evaluation and data splitting\n",
    "### Why do we need to split dataset?\n",
    "We want models learn from data to predict on new data (like the data without label). But whether we should trust their predictions? Thus, we need some methods to evaluate models before using them. Unfortunately, we cannot use the data we used to build the model to evaluate it. This is because our model can always simply remember the whole training set, and will therefore always predict the correct label for any point in the training set. This “remembering” does not indicate to us whether our model will **generalize** well (in other words, whether it will also perform well on new data). To assess the model’s performance, we show it new data (data that it hasn’t seen before) for which we have labels. This is usually done by splitting the labeled data we have collected (here, our 150 flower measurements) into two parts. One part of the data is used to build our machine learning model, and is called the training data or **training set**. The rest of the data will be used to assess how well the model works; this is called the test data, **test set**, or hold-out set. In addition, we will need **valid set** to provide an unbiased evaluation of a model fitted on the training dataset while tuning model hyperparameters. If you have more time, you can read this [article](https://towardsdatascience.com/how-to-split-data-into-three-sets-train-validation-and-test-and-why-e50d22d3e54c) for more details.\n",
    "### Evaluation metrics\n",
    "The metrics used to evaluate the ML models are very important. The choice of metrics to use influences how model performance is measured and compared. The metrics influence both how you weight the importance of different characteristics in the results and your ultimate choice of algorithm. The main evaluation metrics for regression and classification tasks are illustrated below. If you have more time, you can read this [article](https://blog.knoldus.com/model-evaluation-metrics-for-machine-learning-algorithms/) for more details.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://www.oreilly.com/api/v2/epubs/9781492073048/files/assets/mlbf_0407.png\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a633f2c",
   "metadata": {},
   "source": [
    "## Common steps\n",
    "1. Prepare data & split data\n",
    "2. Choose the model\n",
    "3. Train the model\n",
    "4. Evaluate the model\n",
    "5. Use the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048efaf2",
   "metadata": {},
   "source": [
    "## Regression example\n",
    "Below is a simple example to show basic steps of regression tasks. **Our goal** is to build a ML model that can learn from chemical structures (as encoded in SMILES strings) to predict **water solubility**. We will use ESOL dataset from [MoleculeNet](https://doi.org/10.1039/C7SC02664A) to train the models. This dataset contains structures and water solubility data for 1128 compounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a8b2f1",
   "metadata": {},
   "source": [
    "Load dataset & show data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf79eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load dataset from a CSV file\n",
    "esol_df = pd.read_csv('../data/esol.csv')\n",
    "esol_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52e9894",
   "metadata": {},
   "source": [
    "The original dataset contains 2 columns, where the `smiles` column represents the SMILES strings of the solute molecules. The column `log solubility (mol/L)` represents the solubility of molecules in water, which is the predicted target of our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2950f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = esol_df['smiles'].values\n",
    "y = esol_df['log solubility (mol/L)'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343779ed",
   "metadata": {},
   "source": [
    "We need to convert the SMILES strings of molecules into numerical values that can be used as input to the ML models. We can calculate molecular descirptors from SMILES strings by some software like `RDKit`, `DeepChem` and [Mordred](https://github.com/mordred-descriptor/mordred). Here we use DeepChem [Featurizers](https://deepchem.readthedocs.io/en/latest/api_reference/featurizers.html) to compute molecular descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5961b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we use molecular descriptors from RDKit, like molecular weight, number of valence electrons, maximum and minimum partial charge, etc.\n",
    "from deepchem.feat import RDKitDescriptors\n",
    "featurizer = RDKitDescriptors()\n",
    "features = featurizer.featurize(smiles)\n",
    "print(\"Number of molecular descriptors:\", features.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07fca6d",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f23025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Min-Max Normalization of features\n",
    "fea_max = features.max(axis=0)\n",
    "fea_min = features.min(axis=0)\n",
    "fea_norm = (features - fea_min) / (fea_max - fea_min)\n",
    "\n",
    "# Check if normalized features contain invalid values\n",
    "contain_nan = (True in np.isnan(fea_norm))\n",
    "if contain_nan:\n",
    "    print('Our normalized features contain invalid values, please delete them before model training!')\n",
    "    fea_norm = fea_norm[:, ~np.isnan(fea_norm).any(axis=0)]\n",
    "    print('Dropping of columns containing invalid values has been completed.')\n",
    "else:\n",
    "    print('Our normalized features do not contain invalid values.')\n",
    "print(\"Shape of molecular descriptors after data preprocessing:\", fea_norm.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138076f4",
   "metadata": {},
   "source": [
    "Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8675b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = fea_norm\n",
    "# training data size : test data size = 0.8 : 0.2\n",
    "# fixed seed using the random_state parameter, so it always has the same split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394d369a",
   "metadata": {},
   "source": [
    "Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b94772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest regressor, and the default criterion is mean squared error (MSE)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "ranf_reg = RandomForestRegressor(n_estimators=50, random_state=0)  # using 50 trees and seed=0\n",
    "\n",
    "# XGBoost regressor\n",
    "from xgboost import XGBRegressor\n",
    "bst_reg = XGBRegressor(n_estimators=50, random_state=0)  # using 50 trees and seed=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4b6e04",
   "metadata": {},
   "source": [
    "Train and evaluate the models\n",
    "- Mean Squared Error: $MSE$ = $\\frac{1}{n} \\Sigma_{i=1}^n({y}-\\hat{y})^2$\n",
    "- Root Mean Squared Error: $RMSE$ = $\\sqrt{MSE}$ = $\\sqrt{\\frac{1}{n} \\Sigma_{i=1}^n({y}-\\hat{y})^2}$\n",
    "\n",
    "We choose `RMSE` as the evaluation metric for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b21ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for random forests\n",
    "ranf_reg.fit(X_train, y_train)  # train the model\n",
    "ranf_train_mse = ranf_reg.score(X_train, y_train)\n",
    "ranf_test_mse = ranf_reg.score(X_test, y_test)\n",
    "ranf_train_rmse = ranf_train_mse ** 0.5\n",
    "ranf_test_rmse = ranf_test_mse ** 0.5\n",
    "print('Random forests performance:')\n",
    "print('RMSE on train set: {:.3f}, and test set: {:.3f}.\\n'.format(ranf_train_rmse, ranf_test_rmse))\n",
    "\n",
    "# for XGBoost\n",
    "bst_reg.fit(X_train, y_train)  # train the model\n",
    "y_pred_train = bst_reg.predict(X_train)\n",
    "y_pred_test = bst_reg.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "bst_train_mse = mean_squared_error(y_pred_train, y_train)\n",
    "bst_test_mse = mean_squared_error(y_pred_test, y_test)\n",
    "bst_train_rmse = bst_train_mse ** 0.5\n",
    "bst_test_rmse = bst_test_mse ** 0.5\n",
    "print('XGBoost performance:')\n",
    "print('RMSE on train set: {:.3f}, and test set: {:.3f}.'.format(bst_train_rmse, bst_test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712c715f",
   "metadata": {},
   "source": [
    "The results show that the RMSE value of XGBoost on the test set is smaller, indicating that the XGBoost is more accurate than random forests on this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5622c1-38ec-4e45-b6bb-2547a31408f6",
   "metadata": {},
   "source": [
    "# Introduction to traditional ML.\n",
    "\n",
    "## Supervised learning\n",
    "\n",
    "Training a model to take inputs X and return output y.\n",
    "\n",
    "As you have seen in class, for this type of learning, we have two variants:\n",
    "\n",
    "- Classification\n",
    "- Regression\n",
    "\n",
    "Linear regression is one example of suppervised learning for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ee09d-524b-4f90-ac5c-cc37187ba4c4",
   "metadata": {},
   "source": [
    "# Week 2 tutorial - AI 4 Chemistry\n",
    "\n",
    "## Index:\n",
    "\n",
    "- Classification\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4369daf7-90c9-4b68-abb9-ac5d80330dd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Regression\n",
    "\n",
    "### TODO: Improve this introduction based on the ESOL paper, why is solub. prediction important?\n",
    "\n",
    "One problem in both academic and industrial chemistry is predicting solubility. For instance we might know that some molecule has good potential as a ligand for some relevant reaction, however when you synthesize it, you realize it's not soluble under your already optimized reaction conditions! 😥\n",
    "\n",
    "It would be extremely useful to know the solubility of my molecule, **before I even try to synthesize it**!\n",
    "\n",
    "---\n",
    "\n",
    "In this task we will try to solve this using supervised learning. In particular, we will train a regression model using the very convenient [scikit-learn](https://www.kaggle.com/competitions/MerckActivity/data) Python library, to predict solubility based on some molecular descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b43b84-aaf6-48cf-893f-3e42267846bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Let's start with loading the data, visualizing some molecules and their solubility\n",
    "# Let's also see some stats. e.g. size of dataset, distribution of solubility, etc.\n",
    "\n",
    "# TODO: Generate features\n",
    "# TODO do a train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcce8c9-6a29-424b-af64-ed669a5c0070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Let's give an introduction to scikit learn by doing a simple linear regression and see the results.\n",
    "# TODO Introduce sckit-learn, and use a RF model for this.\n",
    "import tempfile\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.neighbors import KNeighborsTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_regression\n",
    "cache_path = tempfile.gettempdir()  # we use a temporary folder here\n",
    "X, _ = make_regression(n_samples=50, n_features=25, random_state=0)\n",
    "estimator = make_pipeline(\n",
    "    KNeighborsTransformer(mode='distance'),\n",
    "    Isomap(n_components=3, metric='precomputed'),\n",
    "    memory=cache_path)\n",
    "X_embedded = estimator.fit_transform(X)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fbad9e-a85a-4e1d-a9fa-39aaa5b31bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Let's train another model (this can be an excercise)\n",
    "\n",
    "# EXERCISE: Implement random forest and and XGBoost models using scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b8ae8d-2a3d-4634-aeb6-3e52df6956fb",
   "metadata": {},
   "source": [
    "# As we see, there are many possible models we can use for this task. But which one is better?\n",
    "\n",
    "In addition, each model has a set of hyperparameters that we need to tune ourselves. How do we select them?\n",
    "\n",
    "This is an important part of machine learning! What we want to know is: What is the best combination of model + model hyperparameters for our task? \n",
    "As you've seen in the course, common strategies to evaluate and compare model's performance include:\n",
    "\n",
    "- Splitting dataset in train/validation/test.\n",
    "- Doing cross-validation for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76a7d2-8e3a-466d-9dad-f83681ede02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split data in train/valid/test\n",
    "\n",
    "# Retrain the models on the train set, and compare them using the validation set.\n",
    "\n",
    "# What model is the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f494286d-5d26-4aa7-9b55-13de0ce0446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Let's do cross-validation\n",
    "\n",
    "# Optimize the hyperparameters for XGBoost, and again compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9494e4-f0bb-441d-981b-65da64b96254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Finally, compare all models on the test set.\n",
    "# Explain that test set should never be seen by models.\n",
    "# This is all completely new data so we know how it would work in real life."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ba8f80-fd59-44f7-beb5-31387ec376d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Classification\n",
    "\n",
    "We now turn our attention towards the other type of supervised learning: classification.\n",
    "\n",
    "Many questions in chemistry can be framed as a classification task: \n",
    "\n",
    "- Will this molecule act as a nucleophile or electrophile in my reaction?\n",
    "- What is the smell of this substance? (fruity, citrus, sweet, ...)\n",
    "\n",
    "<div>\n",
    "<img src=\"img/is_this_a_meme.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "### TODO: Let's get a dataset for classification in molecules. Let's say it's prediction of toxicity.\n",
    "\n",
    "What we want to know is, is the molecule shown toxic or not?\n",
    "\n",
    "##### TODO insert image of molec\n",
    "\n",
    "Let's see if a model can tell what molecules are toxic!\n",
    "\n",
    "This would be very useful for instance in drug discovery, where we want to know if a molecule has potential as a drug, **even before we synthesize it**.\n",
    "\n",
    "## To do this, we will use [mordred](http://mordred-descriptor.github.io/documentation/master/descriptors.html) to generate some molecular descriptors, and will again train some models using scikit-learn using these features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19e380-eaf6-4f72-851c-ccb226f1611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO load dataset\n",
    "# TODO small visualization of dataset, let's see some molecules and the property we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad84a50b-291f-4756-ad52-43907fe4278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO train a Random Forest classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574fdf49-bdd0-4112-8b5e-2d8e62baab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate model using metrics for this (AUC-ROC, accuracy, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede4918c-31a3-4827-81ea-d90935e0e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO explore feature importance\n",
    "# Do these features make sense?\n",
    "# Find out in moldred documentation what the features are and think why this is important for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c462e-0a0b-477c-a7a8-06ae6c613389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO train another model (maybe XGBoost) and explain using SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494863e8-5d37-4e6a-b4df-672e0ba3781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bdd0b1-0584-4dbe-9dc4-7dd295e4042b",
   "metadata": {},
   "source": [
    "# Tasks for today: \n",
    "\n",
    "- scikit learn\n",
    "- classification\n",
    "- regression: [ESOL dataset](https://www.kaggle.com/competitions/MerckActivity/data)\n",
    "- XGBoost: [Merck kaggle challenge](https://www.kaggle.com/competitions/MerckActivity/data)\n",
    "- SHAP values + feature importance\n",
    "- Evaluation of ML models\n",
    "- Cross-validation: hyperparameter tuning\n",
    "- Train/valid/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdfcd5a-2969-4f05-8641-88ba0d413c12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1b1e114f4dae097b9e32029c5d22d73dc21a5dd723446d46774bd2adced9390"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
